# Interpretability Paper

By [YuweiYin](https://github.com/YuweiYin)

---

<h2 id="yyw-directory">Directory</h2>

- Interpretability Paper
  - <a href="#yyw-chapter-interpretability-paper-2010-2015">2010 - 2015</a>
  - <a href="#yyw-chapter-interpretability-paper-2016">2016</a>
  - <a href="#yyw-chapter-interpretability-paper-2017">2017</a>
  - <a href="#yyw-chapter-interpretability-paper-2018">2018</a>
  - <a href="#yyw-chapter-interpretability-paper-2019">2019</a>
  - <a href="#yyw-chapter-interpretability-paper-2020">2020</a>
  - <a href="#yyw-chapter-interpretability-paper-2021">2021</a>

---

<h3 id="yyw-chapter-interpretability-paper-2010-2015">2010 - 2015</h3>

- <a href="#yyw-directory">Back to Directory</a>

- (2013) (ICCV) HOGgles: Visualizing Object Detection Features
- (2014) (ECCV) Visualizing and Understanding Convolutional Networks
- (2014) (ICLR) Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps
- (2015) (AAS) Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model
- (2015) (CVPR) Understanding Deep Image Representations by Inverting Them
- (2015) (ICCV) Understanding deep features with computer-generated imagery
- (2015) (ICLR) Striving for Simplicity: The All Convolutional Net
- (2015) (ICML) Understanding Neural Networks Through Deep Visualization

<h3 id="yyw-chapter-interpretability-paper-2016">2016</h3>

- <a href="#yyw-directory">Back to Directory</a>

- (2016) (ACL) Explaining Predictions of Non-Linear Classifiers in NLP
- (2016) (arXiv) Attentive Explanations: Justifying Decisions and Pointing to the Evidence
- (2016) (arXiv) Grad-CAM: Why did you say that
- (2016) (arXiv) Investigating the influence of noise and distractors on the interpretation of neural networks
- (2016) (arXiv) Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks
- (2016) (arXiv) The Mythos of Model Interpretability
- (2016) (arXiv) Understanding Neural Networks through Representation Erasure
- (2016) (CVPR) Analyzing Classifiers: Fisher Vectors and Deep Neural Networks
- (2016) (CVPR) Inverting Visual Representations with Convolutional Networks
- (2016) (CVPR) Visualizing and Understanding Deep Texture Representations
- (2016) (ECCV) Design of kernels in convolutional neural networks for image classification
- (2016) (ECCV) Generating Visual Explanations
- (2016) (EMNLP) Rationalizing Neural Predictions
- (2016) (ICML) Understanding and improving convolutional neural networks via concatenated rectified linear units
- (2016) (ICML) Visualizing and Comparing AlexNet and VGG using Deconvolutional Layers
- (2016) (IJCV) Visualizing deep convolutional neural networks using natural pre-images
- (2016) (IJCV) Visualizing Object Detection Features
- (2016) (KDD) LIME-Why should i trust you: Explaining the predictions of any classifier
- (2016) (NAACL) Visualizing and Understanding Neural Models in NLP
- (2016) (NIPS) Synthesizing the preferred inputs for neurons in neural networks via deep generator networks
- (2016) (NIPS) Understanding the effective receptive field in deep convolutional neural networks
- (2016) (TVCG) Towards Better Analysis of Deep Convolutional Neural Networks
- (2016) (TVCG) Visualizing the Hidden Activity of Artificial Neural Networks

<h3 id="yyw-chapter-interpretability-paper-2017">2017</h3>

- <a href="#yyw-directory">Back to Directory</a>

- (2017) (AAAI) Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning
- (2017) (ACL) Visualizing and Understanding Neural Machine Translation
- (2017) (arXiv) Contextual Explanation Networks
- (2017) (arXiv) Distilling a neural network into a soft decision tree
- (2017) (arXiv) Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models
- (2017) (arXiv) Interpretable & Explorable Approximations of Black Box Models
- (2017) (arXiv) SmoothGrad: removing noise by adding noise
- (2017) (arXiv) Towards interpretable deep neural networks by leveraging adversarial examples
- (2017) (arXiv) Transparency: Motivations and Challenges
- (2017) (CEURW) What does explainable AI really mean: A new conceptualization of perspectives
- (2017) (CVPR) Improving Interpretability of Deep Neural Networks with Semantic Information
- (2017) (CVPR) Interpretable 3d human action analysis with temporal convolutional networks
- (2017) (CVPR) Knowing when to look: Adaptive attention via a visual sentinel for image captioning
- (2017) (CVPR) Looking under the hood: Deep neural network visualization to interpret whole-slide image analysis outcomes for colorectal polyps
- (2017) (CVPR) Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering
- (2017) (CVPR) MDNet: A Semantically and Visually Interpretable Medical Image Diagnosis Network
- (2017) (CVPR) Mining Object Parts from CNNs via Active Question-Answering
- (2017) (CVPR) Network dissection: Quantifying interpretability of deep visual representations
- (2017) (EMNLP) A causal framework for explaining the predictions of black-box sequence-to-sequence models
- (2017) (ICCV) Grad-cam: Visual explanations from deep networks via gradient-based localization
- (2017) (ICCV) Interpretable Explanations of Black Boxes by Meaningful Perturbation
- (2017) (ICCV) Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention
- (2017) (ICCV) Learning to Disambiguate by Asking Discriminative Questions
- (2017) (ICCV) Understanding and comparing deep neural networks for age and gender classification
- (2017) (ICLR) Exploring LOTS in Deep Neural Networks
- (2017) (ICLR) Visualizing deep neural network decisions: Prediction difference analysis
- (2017) (ICML) Axiomatic Attribution for Deep Networks
- (2017) (ICML) Learning Important Features Through Propagating Activation Differences
- (2017) (ICML) Understanding Black-box Predictions via Influence Functions
- (2017) (IJCAI) Right for the right reasons: Training differentiable models by constraining their explanations
- (2017) (IJCAI) Understanding and improving convolutional neural networks via concatenated rectified linear units
- (2017) () Interpretability of deep learning models: a survey of results
- (2017) (NIPS) A Unified Approach to Interpreting Model Predictions
- (2017) (NIPS) Real Time Image Saliency for Black Box Classifiers
- (2017) (NIPS) SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability
- (2017) (SOSP) Deepxplore: Automated whitebox testing of deep learning systems
- (2017) (TVCG) ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models

<h3 id="yyw-chapter-interpretability-paper-2018">2018</h3>

- <a href="#yyw-directory">Back to Directory</a>

- (2018) (AAAI) Anchors: High-Precision Model-Agnostic Explanations
- (2018) (AAAI) Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions
- (2018) (AAAI) Examining CNN Representations With Respect To Dataset Bias
- (2018) (AAAI) Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients
- (2018) (AAAI) Interpreting CNN Knowledge Via An Explanatory Graph
- (2018) (ACL) Did the Model Understand the Question
- (2018) (ACM-CompSurv) A Survey of Methods for Explaining Black Box Models
- (2018) (arXiv) Computationally Efficient Measures of Internal Neuron Importance
- (2018) (arXiv) Distill-and-Compare: Auditing Black-Box Models Using Transparent Model Distillation
- (2018) (arXiv) How convolutional neural network see the world-A survey of convolutional neural network visualization methods
- (2018) (arXiv) Manipulating and Measuring Model Interpretability
- (2018) (arXiv) Revisiting the importance of individual units in cnns via ablation
- (2018) (arXiv) Unsupervised Learning of Neural Networks to Explain Neural Networks
- (2018) (BMVC) Rise: Randomized input sampling for explanation of black-box models
- (2018) (CommunACM) The Mythos of Model Interpretability
- (2018) (CVPR) Interpret Neural Networks by Identifying Critical Data Routing Paths
- (2018) (CVPR) Interpretable Convolutional Neural Networks
- (2018) (CVPR) Learning to Act Properly: Predicting and Explaining Affordances from Images
- (2018) (CVPR) Multimodal Explanations: Justifying Decisions and Pointing to the Evidence
- (2018) (CVPR) Net2vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks
- (2018) (CVPR) Teaching Categories to Human Learners with Visual Explanations
- (2018) (CVPR) Tell Me Where to Look: Guided Attention Inference Network
- (2018) (CVPR) Transparency by design: Closing the gap between performance and interpretability in visual reasoning
- (2018) (CVPR) What do Deep Networks Like to See
- (2018) (CVPR) What have we learned from deep representations for action recognition
- (2018) (DSP) Methods for Interpreting and Understanding Deep Neural Networks
- (2018) (ECCV) Choose Your Neuron: Incorporating Domain Knowledge through Neuron-Importance
- (2018) (ECCV) Convnets and imagenet beyond accuracy: Understanding mistakes and uncovering biases
- (2018) (ECCV) Deep clustering for unsupervised learning of visual features
- (2018) (ECCV) Diverse feature visualizations reveal invariances in early layers of deep neural networks
- (2018) (ECCV) Explainable neural computation via stack neural module networks
- (2018) (ECCV) ExplainGAN: Model Explanation via Decision Boundary Crossing Transformations
- (2018) (ECCV) Grounding Visual Explanations
- (2018) (ECCV) Interpretable basis decomposition for visual explanation
- (2018) (ECCV) Textual Explanations for Self-Driving Vehicles
- (2018) (ECCV) Vqa-e: Explaining, elaborating, and enhancing your answers for visual questions
- (2018) (FITEE) Visual Interpretability for Deep Learning: a Survey
- (2018) (ICLR) Detecting statistical interactions from neural network weights
- (2018) (ICLR) Interpretable counting for visual question answering
- (2018) (ICLR) Learning how to explain neural networks: PatternNet and PatternAttribution
- (2018) (ICLR) On the importance of single directions for generalization
- (2018) (ICLR) Towards better understanding of gradient-based attribution methods for deep neural networks
- (2018) (ICML) Extracting Automata from Recurrent Neural Networks Using Queries and Counterexamples
- (2018) (ICML) Interpretability beyond feature attribution: Quantitative testing with concept activation vectors
- (2018) (ICML) Learning to explain: An information-theoretic perspective on model interpretation
- (2018) (IEEEacesss) Peeking inside the black-box: A survey on Explainable Artificial Intelligence (XAI)
- (2018) (IJCV) Top-down neural attention by excitation backprop
- (2018) (JAIR) Learning Explanatory Rules from Noisy Data
- (2018) (KDD) Towards Explanation of DNN-based Prediction with Guided Feature Inversion
- (2018) (MIPRO) Explainable Artificial Intelligence: A Survey
- (2018) (NeurIPS) Attacks meet interpretability: Attribute-steered detection of adversarial samples
- (2018) (NeurIPS) DeepPINK: reproducible feature selection in deep neural networks
- (2018) (NeurIPS) Explanations based on the missing: Towards contrastive explanations with pertinent negatives
- (2018) (NeurIPS) Interpretable Convolutional Filters with SincNet
- (2018) (NeurIPS) Representer point selection for explaining deep neural networks
- (2018) (NeurIPS) Sanity Checks for Saliency Maps
- (2018) (NeurIPS) Towards robust interpretability with self-explaining neural networks
- (2018) (TPAMI) Interpreting deep visual representations via network dissection
- (2018) (WACV) Examining CNN Representations With Respect To Dataset Bias

<h3 id="yyw-chapter-interpretability-paper-2019">2019</h3>

- <a href="#yyw-directory">Back to Directory</a>

- (2019) (AAAI) Can You Explain That: Lucid Explanations Help Human-AI Collaborative Image Retrieval
- (2019) (AAAI) Classifier-agnostic saliency map extraction
- (2019) (AAAI) Interpretation of Neural Networks is Fragile
- (2019) (AAAI) Network Transplanting
- (2019) (AAAI) Unsupervised Learning of Neural Networks to Explain Neural Networks
- (2019) (ACL) Attention is not Explanation
- (2019) (ACMFAT) Explaining Explanations in AI
- (2019) (AI) Explanation in Artificial Intelligence: Insights from the Social Sciences
- (2019) (arXiv) Attention Interpretability Across NLP Tasks
- (2019) (arXiv) Interpretable CNNs for Object Classification
- (2019) (CSUR) A Survey of Methods for Explaining Black Box Models
- (2019) (CVPR) Attention branch network: Learning of attention mechanism for visual explanation
- (2019) (CVPR) FickleNet: Weakly and Semi-supervised Semantic Image Segmentation using Stochastic Inference
- (2019) (CVPR) From Recognition to Cognition: Visual Commonsense Reasoning
- (2019) (CVPR) Interpretable and fine-grained visual explanations for convolutional neural networks
- (2019) (CVPR) Interpreting CNNs via Decision Trees
- (2019) (CVPR) Learning to Explain with Complemental Examples
- (2019) (CVPR) Multimodal Explanations by Predicting Counterfactuality in Videos
- (2019) (CVPR) Revealing Scenes by Inverting Structure from Motion Reconstructions
- (2019) (CVPRW) Visualizing the Resilience of Deep Convolutional Network Interpretations
- (2019) (EMNLP) Attention is not not Explanation
- (2019) (ExplainableAI) The (Un)reliability of saliency methods
- (2019) (ICAIS) Interpreting Black Box Predictions using Fisher Kernels
- (2019) (ICCV) Explaining Neural Networks Semantically and Quantitatively
- (2019) (ICCV) Taking a HINT: Leveraging Explanations to Make Vision and Language Models More Grounded
- (2019) (ICCV) Towards Interpretable Face Recognition
- (2019) (ICCV) U-CAM: Visual Explanation using Uncertainty based Class Activation Maps
- (2019) (ICCV) Understanding Deep Networks via Extremal Perturbations and Smooth Masks
- (2019) (ICLR) Hierarchical interpretations for neural network predictions
- (2019) (ICLR) How Important Is a Neuron
- (2019) (ICLR) Visual Explanation by Interpretation: Improving Visual Feedback Capabilities of Deep Neural Networks
- (2019) (ICML) Towards A Deep and Unified Understanding of Deep Neural Models in NLP
- (2019) (JVCIR) Interpretable Convolutional Neural Networks via Feedforward Design
- (2019) (NeurIPS) A benchmark for interpretability methods in deep neural networks
- (2019) (NeurIPS) Can you trust your model's uncertainty: Evaluating predictive uncertainty under dataset shift
- (2019) (NeurIPS) CXPlain: Causal explanations for model interpretation under uncertainty
- (2019) (NeurIPS) Full-gradient representation for neural network visualization
- (2019) (NeurIPS) On the (In) fidelity and Sensitivity of Explanations
- (2019) (NeurIPS) This looks like that: deep learning for interpretable image recognition
- (2019) (NeurIPS) Towards Automatic Concept-based Explanations
- (2019) (NMI) Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead

<h3 id="yyw-chapter-interpretability-paper-2020">2020</h3>

- <a href="#yyw-directory">Back to Directory</a>

- (2020) (arXiv) A Survey on Neural Network Interpretability
- (2020) (CVPR) Explaining Knowledge Distillation by Quantifying the Knowledge
- (2020) (CVPR) High-frequency Component Helps Explain the Generalization of Convolutional Neural Networks
- (2020) (CVPR) Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks
- (2020) (ICLR) Interpretable Complex-Valued Neural Networks for Privacy Protection
- (2020) (ICLR) Knowledge consistency between neural networks and beyond

<h3 id="yyw-chapter-interpretability-paper-2021">2021</h3>

- <a href="#yyw-directory">Back to Directory</a>

- (2021) (AAAI) Self-Attention Attribution: Interpreting Information Interactions Inside Transformer

---
